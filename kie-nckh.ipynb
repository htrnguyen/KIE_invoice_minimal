{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11260059,"sourceType":"datasetVersion","datasetId":7037505},{"sourceId":11263195,"sourceType":"datasetVersion","datasetId":7039895}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/htrnguyen/KIE_invoice_minimal.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:48:14.579239Z","iopub.execute_input":"2025-04-04T06:48:14.579576Z","iopub.status.idle":"2025-04-04T06:48:15.979634Z","shell.execute_reply.started":"2025-04-04T06:48:14.579553Z","shell.execute_reply":"2025-04-04T06:48:15.978869Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'KIE_invoice_minimal'...\nremote: Enumerating objects: 556, done.\u001b[K\nremote: Counting objects: 100% (41/41), done.\u001b[K\nremote: Compressing objects: 100% (33/33), done.\u001b[K\nremote: Total 556 (delta 11), reused 28 (delta 7), pack-reused 515 (from 1)\u001b[K\nReceiving objects: 100% (556/556), 1.14 MiB | 8.91 MiB/s, done.\nResolving deltas: 100% (279/279), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd /kaggle/working/KIE_invoice_minimal","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:48:15.981075Z","iopub.execute_input":"2025-04-04T06:48:15.981398Z","iopub.status.idle":"2025-04-04T06:48:15.991721Z","shell.execute_reply.started":"2025-04-04T06:48:15.981363Z","shell.execute_reply":"2025-04-04T06:48:15.991034Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/KIE_invoice_minimal\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:48:15.993383Z","iopub.execute_input":"2025-04-04T06:48:15.993683Z","iopub.status.idle":"2025-04-04T06:48:16.114993Z","shell.execute_reply.started":"2025-04-04T06:48:15.993655Z","shell.execute_reply":"2025-04-04T06:48:16.114194Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/KIE_invoice_minimal\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q transformers dgl-cu113 huggingface_hub gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:48:16.116468Z","iopub.execute_input":"2025-04-04T06:48:16.116712Z","iopub.status.idle":"2025-04-04T06:48:18.078153Z","shell.execute_reply.started":"2025-04-04T06:48:16.116687Z","shell.execute_reply":"2025-04-04T06:48:18.077291Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement dgl-cu113 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for dgl-cu113\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!cp -r /kaggle/input/kie-dt/images /kaggle/working/KIE_invoice_minimal/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:48:18.079088Z","iopub.execute_input":"2025-04-04T06:48:18.079340Z","iopub.status.idle":"2025-04-04T06:48:33.207079Z","shell.execute_reply.started":"2025-04-04T06:48:18.079316Z","shell.execute_reply":"2025-04-04T06:48:33.206070Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!cp -r /kaggle/input/kie-dt/mcocr_labels.json /kaggle/working/KIE_invoice_minimal/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:48:33.208288Z","iopub.execute_input":"2025-04-04T06:48:33.208686Z","iopub.status.idle":"2025-04-04T06:48:33.338007Z","shell.execute_reply.started":"2025-04-04T06:48:33.208644Z","shell.execute_reply":"2025-04-04T06:48:33.336977Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!python scripts/download_weights.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:48:33.338940Z","iopub.execute_input":"2025-04-04T06:48:33.339167Z","iopub.status.idle":"2025-04-04T06:49:00.253225Z","shell.execute_reply.started":"2025-04-04T06:48:33.339144Z","shell.execute_reply":"2025-04-04T06:49:00.251939Z"}},"outputs":[{"name":"stdout","text":"Downloading CRAFT weights...\nDownloading...\nFrom: https://drive.google.com/uc?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ\nTo: /kaggle/working/KIE_invoice_minimal/weights/text_detect/craft_mlt_25k.pth\n100%|███████████████████████████████████████| 83.2M/83.2M [00:00<00:00, 125MB/s]\nDownloading U2Net weights...\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ\nFrom (redirected): https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ&confirm=t&uuid=1fd5653a-fe93-41a3-a5d4-c19aa8e9700c\nTo: /kaggle/working/KIE_invoice_minimal/weights/saliency/u2netp.pth\n100%|█████████████████████████████████████████| 176M/176M [00:01<00:00, 139MB/s]\nDownloading LayoutXLM weights...\npytorch_model.bin: 100%|████████████████████| 1.48G/1.48G [00:06<00:00, 230MB/s]\nWeights saved to weights/kie/vi_layoutxlm.pth\nAll weights downloaded successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!python scripts/prepare_data.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:49:00.256154Z","iopub.execute_input":"2025-04-04T06:49:00.256415Z","iopub.status.idle":"2025-04-04T06:49:01.759856Z","shell.execute_reply.started":"2025-04-04T06:49:00.256389Z","shell.execute_reply":"2025-04-04T06:49:01.758812Z"}},"outputs":[{"name":"stdout","text":"Loading annotations from data/mcocr_labels.json\nLoaded 209 image annotations\n\nValidating JSON structure...\nFirst few image keys: ['0269.jpg', '0270.jpg', '0271.jpg']\n\nFirst item keys: dict_keys(['cells', 'h_origin', 'w_origin'])\n\nFirst cell structure: {'poly': [770, 1193, 1082, 1192, 1077, 1252, 768, 1240], 'cate_id': 1, 'cate_text': 'brand', 'vietocr_text': 'NHABEXIMS', 'group_id': 0}\n\nAnalyzing data distribution...\n\nLabel distribution in dataset:\nBRAND: 169 instances in 148 images\nEXP: 113 instances in 110 images\nEXP_LABEL: 117 instances in 109 images\nMFG: 110 instances in 110 images\nMFG_LABEL: 109 instances in 105 images\nNAME: 220 instances in 158 images\nOTHER: 49 instances in 43 images\nWEIGHT: 136 instances in 126 images\nWEIGHT_LABEL: 131 instances in 119 images\n\nSplitting dataset...\nTotal number of images: 209\n\nProcessing train split (146 images)...\nSuccessfully processed 146/146 images for train\n\nProcessing val split (19 images)...\nSuccessfully processed 19/19 images for val\n\nProcessing test split (44 images)...\nSuccessfully processed 44/44 images for test\n\nVerifying split distributions...\n\nLabel distribution in dataset:\nBRAND: 119 instances in 103 images\nEXP: 77 instances in 77 images\nEXP_LABEL: 79 instances in 76 images\nMFG: 77 instances in 77 images\nMFG_LABEL: 75 instances in 73 images\nNAME: 155 instances in 109 images\nOTHER: 32 instances in 29 images\nWEIGHT: 99 instances in 90 images\nWEIGHT_LABEL: 94 instances in 85 images\n\nLabel distribution in dataset:\nBRAND: 16 instances in 14 images\nEXP: 11 instances in 10 images\nEXP_LABEL: 13 instances in 10 images\nMFG: 10 instances in 10 images\nMFG_LABEL: 12 instances in 10 images\nNAME: 19 instances in 14 images\nOTHER: 3 instances in 3 images\nWEIGHT: 11 instances in 10 images\nWEIGHT_LABEL: 11 instances in 10 images\n\nLabel distribution in dataset:\nBRAND: 34 instances in 31 images\nEXP: 25 instances in 23 images\nEXP_LABEL: 25 instances in 23 images\nMFG: 23 instances in 23 images\nMFG_LABEL: 22 instances in 22 images\nNAME: 46 instances in 35 images\nOTHER: 14 instances in 11 images\nWEIGHT: 26 instances in 26 images\nWEIGHT_LABEL: 26 instances in 24 images\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!ls -R data/dataset/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:49:01.761148Z","iopub.execute_input":"2025-04-04T06:49:01.761402Z","iopub.status.idle":"2025-04-04T06:49:05.064380Z","shell.execute_reply.started":"2025-04-04T06:49:01.761378Z","shell.execute_reply":"2025-04-04T06:49:05.063551Z"}},"outputs":[{"name":"stdout","text":"data/dataset/:\nannotations  test  train  val\n\ndata/dataset/annotations:\ntest.json  train.json  val.json\n\ndata/dataset/test:\nimages\n\ndata/dataset/test/images:\n0269.jpg  0292.jpg  0338.jpg  0370.jpg\t0403.jpg  0436.jpg  0461.jpg  0478.jpg\n0271.jpg  0293.jpg  0342.jpg  0371.jpg\t0410.jpg  0437.jpg  0462.jpg  0493.jpg\n0282.jpg  0311.jpg  0352.jpg  0387.jpg\t0414.jpg  0449.jpg  0464.jpg\n0288.jpg  0314.jpg  0359.jpg  0393.jpg\t0415.jpg  0453.jpg  0469.jpg\n0290.jpg  0315.jpg  0361.jpg  0394.jpg\t0423.jpg  0454.jpg  0473.jpg\n0291.jpg  0337.jpg  0367.jpg  0395.jpg\t0430.jpg  0456.jpg  0475.jpg\n\ndata/dataset/train:\nimages\n\ndata/dataset/train/images:\n0270.jpg  0303.jpg  0333.jpg  0363.jpg\t0388.jpg  0416.jpg  0445.jpg  0479.jpg\n0272.jpg  0304.jpg  0334.jpg  0365.jpg\t0389.jpg  0417.jpg  0446.jpg  0480.jpg\n0274.jpg  0305.jpg  0335.jpg  0366.jpg\t0390.jpg  0418.jpg  0447.jpg  0481.jpg\n0276.jpg  0306.jpg  0336.jpg  0368.jpg\t0391.jpg  0419.jpg  0448.jpg  0482.jpg\n0277.jpg  0307.jpg  0339.jpg  0369.jpg\t0396.jpg  0420.jpg  0450.jpg  0483.jpg\n0279.jpg  0308.jpg  0340.jpg  0372.jpg\t0397.jpg  0421.jpg  0455.jpg  0484.jpg\n0280.jpg  0310.jpg  0341.jpg  0373.jpg\t0398.jpg  0422.jpg  0457.jpg  0485.jpg\n0281.jpg  0313.jpg  0343.jpg  0374.jpg\t0400.jpg  0425.jpg  0458.jpg  0486.jpg\n0286.jpg  0316.jpg  0344.jpg  0375.jpg\t0401.jpg  0426.jpg  0460.jpg  0488.jpg\n0287.jpg  0317.jpg  0345.jpg  0377.jpg\t0402.jpg  0427.jpg  0463.jpg  0489.jpg\n0289.jpg  0319.jpg  0348.jpg  0378.jpg\t0404.jpg  0429.jpg  0465.jpg  0490.jpg\n0294.jpg  0320.jpg  0353.jpg  0379.jpg\t0405.jpg  0431.jpg  0466.jpg  0491.jpg\n0295.jpg  0322.jpg  0354.jpg  0380.jpg\t0406.jpg  0433.jpg  0467.jpg  0492.jpg\n0296.jpg  0323.jpg  0355.jpg  0381.jpg\t0407.jpg  0434.jpg  0468.jpg\n0297.jpg  0324.jpg  0356.jpg  0382.jpg\t0408.jpg  0438.jpg  0470.jpg\n0298.jpg  0326.jpg  0357.jpg  0383.jpg\t0409.jpg  0439.jpg  0471.jpg\n0300.jpg  0328.jpg  0358.jpg  0384.jpg\t0411.jpg  0442.jpg  0472.jpg\n0301.jpg  0330.jpg  0360.jpg  0385.jpg\t0412.jpg  0443.jpg  0474.jpg\n0302.jpg  0332.jpg  0362.jpg  0386.jpg\t0413.jpg  0444.jpg  0476.jpg\n\ndata/dataset/val:\nimages\n\ndata/dataset/val/images:\n0283.jpg  0321.jpg  0349.jpg  0364.jpg\t0424.jpg  0451.jpg  0487.jpg\n0312.jpg  0327.jpg  0350.jpg  0392.jpg\t0435.jpg  0452.jpg\n0318.jpg  0331.jpg  0351.jpg  0399.jpg\t0441.jpg  0477.jpg\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 2. Cài đặt PyTorch và torchvision\n!pip install torch==2.0.1 torchvision==0.15.2 -q\n\n# 3. Cài đặt torchdata\n!pip install torchdata==0.6.0 -q \n\n# 4. Cài đặt DGL\n!pip install dgl==1.1.0 -q\n\n# 5. Cài đặt transformers\n!pip install transformers==4.30.0 -q\n\n# 7. Cài đặt các dependencies còn lại\n!pip install -r requirements.txt -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:49:05.065304Z","iopub.execute_input":"2025-04-04T06:49:05.065603Z","iopub.status.idle":"2025-04-04T06:52:35.201319Z","shell.execute_reply.started":"2025-04-04T06:49:05.065576Z","shell.execute_reply":"2025-04-04T06:52:35.200490Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m871.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.5.0.post0 requires torch>=2.1.0, but you have torch 2.0.1 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.5.0.post0 requires torch>=2.1.0, but you have torch 2.0.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\ntorchvision 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.30.0 which is incompatible.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Cannot install -r requirements.txt (line 2), torch==2.0.1 and torchdata==0.6.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install vietocr -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:52:35.202368Z","iopub.execute_input":"2025-04-04T06:52:35.202639Z","iopub.status.idle":"2025-04-04T06:52:53.505616Z","shell.execute_reply.started":"2025-04-04T06:52:35.202610Z","shell.execute_reply":"2025-04-04T06:52:53.504517Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for prefetch-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.0 which is incompatible.\ntorchvision 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!git fetch --all\n!git reset --hard origin/main  \n!git pull origin master","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:52:53.506651Z","iopub.execute_input":"2025-04-04T06:52:53.506992Z","iopub.status.idle":"2025-04-04T06:52:54.456412Z","shell.execute_reply.started":"2025-04-04T06:52:53.506963Z","shell.execute_reply":"2025-04-04T06:52:54.455497Z"}},"outputs":[{"name":"stdout","text":"Fetching origin\nfatal: ambiguous argument 'origin/main': unknown revision or path not in the working tree.\nUse '--' to separate paths from revisions, like this:\n'git <command> [<revision>...] -- [<file>...]'\nFrom https://github.com/htrnguyen/KIE_invoice_minimal\n * branch            master     -> FETCH_HEAD\nAlready up to date.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!python scripts/train_kie.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:52:54.457387Z","iopub.execute_input":"2025-04-04T06:52:54.457598Z","iopub.status.idle":"2025-04-04T06:54:36.450818Z","shell.execute_reply.started":"2025-04-04T06:52:54.457576Z","shell.execute_reply":"2025-04-04T06:54:36.449969Z"}},"outputs":[{"name":"stdout","text":"DGL backend not selected or invalid.  Assuming PyTorch for now.\nSetting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\nUsing device: cuda\nInitializing model...\nconfig.json: 100%|█████████████████████████████| 856/856 [00:00<00:00, 3.53MB/s]\nmodel.safetensors: 100%|██████████████████████| 501M/501M [00:02<00:00, 222MB/s]\nvocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 4.28MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 33.0MB/s]\ntokenizer_config.json: 100%|███████████████| 1.14k/1.14k [00:00<00:00, 7.76MB/s]\nLoading datasets...\n\nStarting training...\n==================================================\n\nTraining for 20 epochs...\n==================================================\n\nEpoch 1/20\n------------------------------\nEpoch 1: 100%|██████████| 73/73 [00:03<00:00, 19.79it/s, loss=0.2032, acc=9.75%]\nTraining - Loss: 2.2267, Accuracy: 9.75%\nValidating Epoch 1: 100%|███████████████████████| 10/10 [00:00<00:00, 21.89it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 2/20\n------------------------------\nEpoch 2: 100%|██████████| 73/73 [00:03<00:00, 22.73it/s, loss=0.2033, acc=9.75%]\nTraining - Loss: 2.2281, Accuracy: 9.75%\nValidating Epoch 2: 100%|███████████████████████| 10/10 [00:00<00:00, 25.22it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 3/20\n------------------------------\nEpoch 3: 100%|██████████| 73/73 [00:03<00:00, 19.29it/s, loss=0.2033, acc=9.62%]\nTraining - Loss: 2.2276, Accuracy: 9.62%\nValidating Epoch 3: 100%|███████████████████████| 10/10 [00:00<00:00, 24.19it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 4/20\n------------------------------\nEpoch 4: 100%|██████████| 73/73 [00:03<00:00, 19.52it/s, loss=0.2033, acc=9.62%]\nTraining - Loss: 2.2284, Accuracy: 9.62%\nValidating Epoch 4: 100%|███████████████████████| 10/10 [00:00<00:00, 15.75it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 5/20\n------------------------------\nEpoch 5: 100%|██████████| 73/73 [00:03<00:00, 23.20it/s, loss=0.2036, acc=9.75%]\nTraining - Loss: 2.2314, Accuracy: 9.75%\nValidating Epoch 5: 100%|███████████████████████| 10/10 [00:00<00:00, 25.37it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 6/20\n------------------------------\nEpoch 6: 100%|██████████| 73/73 [00:03<00:00, 22.93it/s, loss=0.2034, acc=9.75%]\nTraining - Loss: 2.2285, Accuracy: 9.75%\nValidating Epoch 6: 100%|███████████████████████| 10/10 [00:00<00:00, 24.17it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 7/20\n------------------------------\nEpoch 7: 100%|██████████| 73/73 [00:03<00:00, 22.82it/s, loss=0.2035, acc=9.75%]\nTraining - Loss: 2.2304, Accuracy: 9.75%\nValidating Epoch 7: 100%|███████████████████████| 10/10 [00:00<00:00, 26.00it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\nEpoch 00007: reducing learning rate of group 0 to 5.0000e-05.\n\nEpoch 8/20\n------------------------------\nEpoch 8: 100%|██████████| 73/73 [00:03<00:00, 22.60it/s, loss=0.2033, acc=9.75%]\nTraining - Loss: 2.2276, Accuracy: 9.75%\nValidating Epoch 8: 100%|███████████████████████| 10/10 [00:00<00:00, 25.45it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 9/20\n------------------------------\nEpoch 9: 100%|██████████| 73/73 [00:03<00:00, 22.53it/s, loss=0.2034, acc=9.75%]\nTraining - Loss: 2.2288, Accuracy: 9.75%\nValidating Epoch 9: 100%|███████████████████████| 10/10 [00:00<00:00, 24.67it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 10/20\n------------------------------\nEpoch 10: 100%|█████████| 73/73 [00:03<00:00, 23.17it/s, loss=0.2033, acc=9.75%]\nTraining - Loss: 2.2275, Accuracy: 9.75%\nValidating Epoch 10: 100%|██████████████████████| 10/10 [00:00<00:00, 25.04it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 11/20\n------------------------------\nEpoch 11: 100%|█████████| 73/73 [00:03<00:00, 22.12it/s, loss=0.2032, acc=9.75%]\nTraining - Loss: 2.2267, Accuracy: 9.75%\nValidating Epoch 11: 100%|██████████████████████| 10/10 [00:00<00:00, 25.73it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 12/20\n------------------------------\nEpoch 12: 100%|█████████| 73/73 [00:03<00:00, 23.27it/s, loss=0.2035, acc=9.88%]\nTraining - Loss: 2.2296, Accuracy: 9.88%\nValidating Epoch 12: 100%|██████████████████████| 10/10 [00:00<00:00, 25.27it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 13/20\n------------------------------\nEpoch 13: 100%|█████████| 73/73 [00:03<00:00, 23.06it/s, loss=0.2035, acc=9.75%]\nTraining - Loss: 2.2301, Accuracy: 9.75%\nValidating Epoch 13: 100%|██████████████████████| 10/10 [00:00<00:00, 24.97it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\nEpoch 00013: reducing learning rate of group 0 to 2.5000e-05.\n\nEpoch 14/20\n------------------------------\nEpoch 14: 100%|█████████| 73/73 [00:03<00:00, 22.76it/s, loss=0.2030, acc=9.62%]\nTraining - Loss: 2.2244, Accuracy: 9.62%\nValidating Epoch 14: 100%|██████████████████████| 10/10 [00:00<00:00, 26.46it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 15/20\n------------------------------\nEpoch 15: 100%|█████████| 73/73 [00:03<00:00, 23.37it/s, loss=0.2036, acc=9.75%]\nTraining - Loss: 2.2309, Accuracy: 9.75%\nValidating Epoch 15: 100%|██████████████████████| 10/10 [00:00<00:00, 24.56it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 16/20\n------------------------------\nEpoch 16: 100%|█████████| 73/73 [00:03<00:00, 22.57it/s, loss=0.2034, acc=9.75%]\nTraining - Loss: 2.2286, Accuracy: 9.75%\nValidating Epoch 16: 100%|██████████████████████| 10/10 [00:00<00:00, 25.58it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 17/20\n------------------------------\nEpoch 17: 100%|█████████| 73/73 [00:03<00:00, 22.92it/s, loss=0.2032, acc=9.75%]\nTraining - Loss: 2.2269, Accuracy: 9.75%\nValidating Epoch 17: 100%|██████████████████████| 10/10 [00:00<00:00, 24.90it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 18/20\n------------------------------\nEpoch 18: 100%|█████████| 73/73 [00:03<00:00, 20.73it/s, loss=0.2031, acc=9.75%]\nTraining - Loss: 2.2252, Accuracy: 9.75%\nValidating Epoch 18: 100%|██████████████████████| 10/10 [00:00<00:00, 25.53it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nEpoch 19/20\n------------------------------\nEpoch 19: 100%|█████████| 73/73 [00:03<00:00, 23.08it/s, loss=0.2034, acc=9.62%]\nTraining - Loss: 2.2285, Accuracy: 9.62%\nValidating Epoch 19: 100%|██████████████████████| 10/10 [00:00<00:00, 19.46it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\nEpoch 00019: reducing learning rate of group 0 to 1.2500e-05.\n\nEpoch 20/20\n------------------------------\nEpoch 20: 100%|█████████| 73/73 [00:03<00:00, 20.57it/s, loss=0.2030, acc=9.75%]\nTraining - Loss: 2.2250, Accuracy: 9.75%\nValidating Epoch 20: 100%|██████████████████████| 10/10 [00:00<00:00, 24.07it/s]\nValidation - Loss: 2.2239, Accuracy: 9.43%\n\nTraining completed!\n==================================================\n\nTraining completed!\n==================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!python scripts/inference_kie.py --image ./data/images/0097.jpg ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:07:28.824485Z","iopub.execute_input":"2025-04-04T07:07:28.824876Z","iopub.status.idle":"2025-04-04T07:07:52.024935Z","shell.execute_reply.started":"2025-04-04T07:07:28.824844Z","shell.execute_reply":"2025-04-04T07:07:52.023876Z"}},"outputs":[{"name":"stdout","text":"Loading models...\nUsing device: cuda\nModel weight /tmp/vgg_seq2seq.pth exsits. Ignore download!\nLoaded 0/321 layers from pretrained weights\n\nProcessing: 0097.jpg\n==================================================\n\n1. Background Removal & Text Detection\n------------------------------\n- Function make_warp_img run in 0.07275677's\n\n2. Text Recognition\n------------------------------\n- Function run_ocr run in 0.4230895's\n- Function get_group_text_line run in 0.09615922's\n- Function create_merge_cells run in 0.00050545's\n\n3. Information Extraction\n------------------------------\n- Function run_predict run in 0.04609013's\n- Function postprocess_scores run in 0.00049067's\n- Function postprocess_write_info run in 0.0001483's\n\nExtracted Information:\n------------------------------\nMFG_LABEL: a s E g s s\n\nVisualizing Results:\n------------------------------\nẢnh đã được lưu tại: visualization_result.png\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}